{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n",
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (output): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n",
      "进行第0个epoch\n",
      "Epoch:  0 | train loss: 2.3034 | test accuracy: 0.12\n",
      "Epoch:  0 | train loss: 0.4083 | test accuracy: 0.83\n",
      "Epoch:  0 | train loss: 0.5555 | test accuracy: 0.87\n",
      "Epoch:  0 | train loss: 0.2240 | test accuracy: 0.90\n",
      "Epoch:  0 | train loss: 0.1277 | test accuracy: 0.94\n",
      "Epoch:  0 | train loss: 0.1645 | test accuracy: 0.94\n",
      "Epoch:  0 | train loss: 0.0555 | test accuracy: 0.95\n",
      "Epoch:  0 | train loss: 0.1262 | test accuracy: 0.95\n",
      "Epoch:  0 | train loss: 0.0351 | test accuracy: 0.96\n",
      "Epoch:  0 | train loss: 0.1001 | test accuracy: 0.96\n",
      "Epoch:  0 | train loss: 0.1830 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.0948 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.0185 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.1478 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.0928 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.2121 | test accuracy: 0.96\n",
      "Epoch:  0 | train loss: 0.0375 | test accuracy: 0.96\n",
      "Epoch:  0 | train loss: 0.0509 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.1003 | test accuracy: 0.98\n",
      "Epoch:  0 | train loss: 0.0207 | test accuracy: 0.98\n",
      "Epoch:  0 | train loss: 0.1948 | test accuracy: 0.98\n",
      "Epoch:  0 | train loss: 0.0299 | test accuracy: 0.98\n",
      "Epoch:  0 | train loss: 0.0222 | test accuracy: 0.98\n",
      "Epoch:  0 | train loss: 0.0778 | test accuracy: 0.97\n",
      "进行第1个epoch\n",
      "Epoch:  1 | train loss: 0.0338 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0361 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0102 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.1576 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0984 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0309 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0090 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0134 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0380 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0319 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0259 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0115 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0478 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0402 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0201 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0261 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0395 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0298 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0094 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0488 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0888 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0370 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0051 | test accuracy: 0.98\n",
      "Epoch:  1 | train loss: 0.0387 | test accuracy: 0.99\n",
      "进行第2个epoch\n",
      "Epoch:  2 | train loss: 0.0450 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0045 | test accuracy: 0.99\n",
      "Epoch:  2 | train loss: 0.1410 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0334 | test accuracy: 0.99\n",
      "Epoch:  2 | train loss: 0.0143 | test accuracy: 0.99\n",
      "Epoch:  2 | train loss: 0.0489 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0013 | test accuracy: 0.99\n",
      "Epoch:  2 | train loss: 0.1107 | test accuracy: 0.99\n",
      "Epoch:  2 | train loss: 0.0160 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0121 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0058 | test accuracy: 0.99\n",
      "Epoch:  2 | train loss: 0.0330 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0468 | test accuracy: 0.99\n",
      "Epoch:  2 | train loss: 0.0166 | test accuracy: 0.99\n",
      "Epoch:  2 | train loss: 0.0263 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0036 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0257 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0653 | test accuracy: 0.99\n",
      "Epoch:  2 | train loss: 0.0248 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0338 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0149 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0043 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0140 | test accuracy: 0.98\n",
      "Epoch:  2 | train loss: 0.0053 | test accuracy: 0.99\n",
      "进行第3个epoch\n",
      "Epoch:  3 | train loss: 0.0213 | test accuracy: 0.98\n",
      "Epoch:  3 | train loss: 0.0270 | test accuracy: 0.99\n",
      "Epoch:  3 | train loss: 0.0022 | test accuracy: 0.99\n",
      "Epoch:  3 | train loss: 0.0199 | test accuracy: 0.98\n",
      "Epoch:  3 | train loss: 0.0024 | test accuracy: 0.99\n",
      "Epoch:  3 | train loss: 0.0061 | test accuracy: 0.99\n",
      "Epoch:  3 | train loss: 0.0215 | test accuracy: 0.99\n",
      "Epoch:  3 | train loss: 0.1098 | test accuracy: 0.98\n",
      "Epoch:  3 | train loss: 0.0049 | test accuracy: 0.99\n",
      "Epoch:  3 | train loss: 0.0687 | test accuracy: 0.99\n",
      "Epoch:  3 | train loss: 0.0263 | test accuracy: 0.99\n",
      "Epoch:  3 | train loss: 0.0082 | test accuracy: 0.98\n",
      "Epoch:  3 | train loss: 0.0086 | test accuracy: 0.98\n",
      "Epoch:  3 | train loss: 0.0223 | test accuracy: 0.98\n",
      "Epoch:  3 | train loss: 0.0220 | test accuracy: 0.98\n",
      "Epoch:  3 | train loss: 0.0049 | test accuracy: 0.99\n",
      "Epoch:  3 | train loss: 0.0072 | test accuracy: 0.98\n",
      "Epoch:  3 | train loss: 0.0678 | test accuracy: 0.98\n",
      "Epoch:  3 | train loss: 0.0023 | test accuracy: 0.98\n",
      "Epoch:  3 | train loss: 0.1764 | test accuracy: 0.99\n",
      "Epoch:  3 | train loss: 0.0048 | test accuracy: 0.99\n",
      "Epoch:  3 | train loss: 0.0365 | test accuracy: 0.98\n",
      "Epoch:  3 | train loss: 0.0513 | test accuracy: 0.99\n",
      "Epoch:  3 | train loss: 0.0115 | test accuracy: 0.98\n",
      "进行第4个epoch\n",
      "Epoch:  4 | train loss: 0.0086 | test accuracy: 0.98\n",
      "Epoch:  4 | train loss: 0.0063 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0278 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0294 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0124 | test accuracy: 0.98\n",
      "Epoch:  4 | train loss: 0.0318 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0093 | test accuracy: 0.98\n",
      "Epoch:  4 | train loss: 0.0223 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0015 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0221 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.1905 | test accuracy: 0.98\n",
      "Epoch:  4 | train loss: 0.0250 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0239 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0150 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0399 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0025 | test accuracy: 0.98\n",
      "Epoch:  4 | train loss: 0.0046 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0044 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0497 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0380 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0279 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0052 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0036 | test accuracy: 0.99\n",
      "Epoch:  4 | train loss: 0.0579 | test accuracy: 0.98\n",
      "进行第5个epoch\n",
      "Epoch:  5 | train loss: 0.0534 | test accuracy: 0.98\n",
      "Epoch:  5 | train loss: 0.0041 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0025 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0814 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0021 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0578 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0099 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0887 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0008 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0114 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0021 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0489 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0025 | test accuracy: 0.98\n",
      "Epoch:  5 | train loss: 0.0014 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.1529 | test accuracy: 0.98\n",
      "Epoch:  5 | train loss: 0.0432 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0009 | test accuracy: 0.98\n",
      "Epoch:  5 | train loss: 0.0441 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0009 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0032 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0339 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0039 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0033 | test accuracy: 0.99\n",
      "Epoch:  5 | train loss: 0.0022 | test accuracy: 0.99\n",
      "进行第6个epoch\n",
      "Epoch:  6 | train loss: 0.0024 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0015 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0331 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0142 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0182 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0054 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0038 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0324 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0709 | test accuracy: 0.98\n",
      "Epoch:  6 | train loss: 0.0251 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0137 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0029 | test accuracy: 0.98\n",
      "Epoch:  6 | train loss: 0.0025 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0154 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0025 | test accuracy: 0.98\n",
      "Epoch:  6 | train loss: 0.0047 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0003 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0035 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0042 | test accuracy: 0.98\n",
      "Epoch:  6 | train loss: 0.0833 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0044 | test accuracy: 0.98\n",
      "Epoch:  6 | train loss: 0.0001 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0123 | test accuracy: 0.99\n",
      "Epoch:  6 | train loss: 0.0012 | test accuracy: 0.99\n",
      "进行第7个epoch\n",
      "Epoch:  7 | train loss: 0.0031 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0201 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0095 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0202 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0032 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0006 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0026 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0010 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0002 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.1032 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0025 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0047 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0169 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0001 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0022 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0011 | test accuracy: 0.98\n",
      "Epoch:  7 | train loss: 0.1335 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0010 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0145 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0257 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0048 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0001 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0008 | test accuracy: 0.99\n",
      "Epoch:  7 | train loss: 0.0123 | test accuracy: 0.99\n",
      "进行第8个epoch\n",
      "Epoch:  8 | train loss: 0.0006 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0226 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0002 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0080 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0028 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0887 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0108 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0002 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0016 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0007 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0169 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0021 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0019 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0044 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0007 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0012 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0002 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0010 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0021 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0000 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0015 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0009 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0064 | test accuracy: 0.99\n",
      "Epoch:  8 | train loss: 0.0002 | test accuracy: 0.98\n",
      "进行第9个epoch\n",
      "Epoch:  9 | train loss: 0.0002 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0061 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0015 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0218 | test accuracy: 0.98\n",
      "Epoch:  9 | train loss: 0.0001 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0006 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0001 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0000 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0018 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0146 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0014 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0173 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0018 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0008 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0010 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0005 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0003 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0126 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0046 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.1005 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0021 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0111 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0011 | test accuracy: 0.99\n",
      "Epoch:  9 | train loss: 0.0056 | test accuracy: 0.99\n",
      "[7 2 1 0 4 1 4 9 5 9]\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    作者：Troublemaker\n",
    "    功能：\n",
    "    版本：\n",
    "    日期：2020/4/5 19:57\n",
    "    脚本：cnn.py\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)\n",
    "# 设置超参数\n",
    "epoches = 10\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 搭建CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()   # 继承__init__功能\n",
    "        ## 第一层卷积\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # 输入[1,28,28]\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,    # 输入图片的高度\n",
    "                out_channels=16,  # 输出图片的高度\n",
    "                kernel_size=5,    # 5x5的卷积核，相当于过滤器\n",
    "                stride=1,         # 卷积核在图上滑动，每隔一个扫一次\n",
    "                padding=2,        # 给图外边补上0\n",
    "            ),\n",
    "            # 经过卷积层 输出[16,28,28] 传入池化层\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)   # 经过池化 输出[16,14,14] 传入下一个卷积\n",
    "        )\n",
    "        ## 第二层卷积\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,    # 同上\n",
    "                out_channels=32,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            # 经过卷积 输出[32, 14, 14] 传入池化层\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)  # 经过池化 输出[32,7,7] 传入输出层\n",
    "        )\n",
    "        ## 输出层\n",
    "        self.output = nn.Linear(in_features=32*7*7, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)           # [batch, 32,7,7]\n",
    "        x = x.view(x.size(0), -1)   # 保留batch, 将后面的乘到一起 [batch, 32*7*7]\n",
    "        output = self.output(x)     # 输出[50,10]\n",
    "        return output\n",
    "\n",
    "\n",
    "# 下载MNist数据集\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"./mnist/\",  # 训练数据保存路径\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),  # 数据范围已从(0-255)压缩到(0,1)\n",
    "    download=True,  # 是否需要下载\n",
    ")\n",
    "# print(train_data.train_data.size())   # [60000,28,28]\n",
    "# print(train_data.train_labels.size())  # [60000]\n",
    "# plt.imshow(train_data.train_data[0].numpy())\n",
    "# plt.show()\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root=\"./mnist/\", train=False)\n",
    "print(test_data.test_data.size())    # [10000, 28, 28]\n",
    "# print(test_data.test_labels.size())  # [10000, 28, 28]\n",
    "test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255\n",
    "test_y = test_data.test_labels[:2000]\n",
    "\n",
    "# 装入Loader中\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # cnn 实例化\n",
    "    cnn = CNN()\n",
    "    print(cnn)\n",
    "\n",
    "    # 定义优化器和损失函数\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 开始训练\n",
    "    for epoch in range(epoches):\n",
    "        print(\"进行第{}个epoch\".format(epoch))\n",
    "        for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            output = cnn(batch_x)  # batch_x=[50,1,28,28]\n",
    "            # output = output[0]\n",
    "            loss = loss_function(output, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                test_output = cnn(test_x)  # [10000 ,10]\n",
    "                pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "                # accuracy = sum(pred_y==test_y)/test_y.size(0)\n",
    "                accuracy = ((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))\n",
    "                print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\n",
    "\n",
    "\n",
    "    test_output = cnn(test_x[:10])\n",
    "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "    print(pred_y)\n",
    "    print(test_y[:10])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xugx\\AppData\\Local\\Temp\\ipykernel_11160\\1698328179.py:33: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000]/255.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mf:\\My_work_file\\Program_file\\VScode_Pro\\python_pro\\Graduation_Design\\DL\\demo\\manual_seed.ipynb 单元格 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W1sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m test_x \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39munsqueeze(test_data\u001b[39m.\u001b[39mtest_data, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), volatile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mFloatTensor)[:\u001b[39m2000\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m255.\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# torch.unsqueeze 返回一个新的张量，对输入的既定位置插入维度 1\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m test_y \u001b[39m=\u001b[39m test_data[:\u001b[39m2000\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# 数据预处理\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W1sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m  \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m  \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W1sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# 4）需要·实现·forward()方法，用于网络的前向传播，而反向传播只需要·调用·Variable.backward()即可。\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W1sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# 输入的四维张量[N, C, H, W]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W1sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCNN\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n",
      "File \u001b[1;32mf:\\Program\\Programming\\Python\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:138\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m    131\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m        index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39m        tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     img, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index], \u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets[index])\n\u001b[0;32m    140\u001b[0m     \u001b[39m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[39m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import torch # 需要的各种包\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision # 数据库模块\n",
    " \n",
    " \n",
    "# 数据预处理\n",
    "# 将training data转化成torch能够使用的DataLoader，这样可以方便使用batch进行训练\n",
    "torch.manual_seed(1) # reproducible 将随机数生成器的种子设置为固定值，这样，当调用时torch.rand(x)，结果将可重现\n",
    " \n",
    "# Hyper Parameters\n",
    "EPOCH = 1 # 训练迭代次数\n",
    "BATCH_SIZE = 50 # 分块送入训练器\n",
    "LR = 0.001 # 学习率 learning rate\n",
    " \n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist/', # 保存位置 若没有就新建\n",
    "    train=True, # training set\n",
    "    transform=torchvision.transforms.ToTensor(), # \n",
    "    # converts a PIL.Image or numpy.ndarray to torch.FloatTensor(C*H*W) in range(0.0,1.0)\n",
    "    download=True\n",
    ")\n",
    " \n",
    "test_data = torchvision.datasets.MNIST(root='./MNIST/')\n",
    " \n",
    "# 如果是普通的Tensor数据，想使用 torch_dataset = data.TensorDataset(data_tensor=x, target_tensor=y)\n",
    "# 将Tensor转换成torch能识别的dataset\n",
    "# 批训练， 50 samples, 1 channel, 28*28, (50, 1, 28 ,28)\n",
    "train_loader = data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    " \n",
    "test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000]/255.\n",
    "# torch.unsqueeze 返回一个新的张量，对输入的既定位置插入维度 1\n",
    " \n",
    "test_y = test_data.test_lables[:2000]\n",
    "# 数据预处理\n",
    " \n",
    " \n",
    "# 定义网络结构\n",
    "# 1）class CNN需要·继承·Module \n",
    "# 2）需要·调用·父类的构造方法：super(CNN, self).__init__()\n",
    "# 3）在Pytorch中激活函数Relu也算是一层layer\n",
    "# 4）需要·实现·forward()方法，用于网络的前向传播，而反向传播只需要·调用·Variable.backward()即可。\n",
    "# 输入的四维张量[N, C, H, W]\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # nn.Sequential一个有序的容器，神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行，\n",
    "        # 同时以神经网络模块为元素的有序字典也可以作为传入参数\n",
    "        # nn.Conv2d 二维卷积 先实例化再使用 在Pytorch的nn模块中，它是不需要你手动定义网络层的权重和偏置的\n",
    "        self.conv1 = nn.Sequential( #input shape (1,28,28)\n",
    "            nn.Conv2d(in_channels=1, #input height 必须手动提供 输入张量的channels数\n",
    "                      out_channels=16, #n_filter 必须手动提供 输出张量的channels数\n",
    "                      kernel_size=5, #filter size 必须手动提供 卷积核的大小 \n",
    "                      # 如果左右两个数不同，比如3x5的卷积核，那么写作kernel_size = (3, 5)，注意需要写一个tuple，而不能写一个列表（list）\n",
    "                      stride=1, #filter step 卷积核在图像窗口上每次平移的间隔，即所谓的步长\n",
    "                      padding=2 #con2d出来的图片大小不变 Pytorch与Tensorflow在卷积层实现上最大的差别就在于padding上\n",
    "            ), # output shape (16,28,28) 输出图像尺寸计算公式是唯一的 # O = （I - K + 2P）/ S +1\n",
    "            nn.ReLU(), # 分段线性函数，把所有的负值都变为0，而正值不变，即单侧抑制\n",
    "            nn.MaxPool2d(kernel_size=2) #2x2采样，28/2=14，output shape (16,14,14) maxpooling有局部不变性而且可以提取显著特征的同时降低模型的参数，从而降低模型的过拟合\n",
    "        ) \n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, 5, 1, 2), #output shape (32,7,7)\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(2))\n",
    "        # 因上述几层网络处理后的output为[32,7,7]的tensor，展开即为7*7*32的一维向量，接上一层全连接层，最终output_size应为10，即识别出来的数字总类别数\n",
    "        # 在二维图像处理的任务中，全连接层的输入与输出一般都设置为二维张量，形状通常为[batch_size, size]\n",
    "        self.out = nn.Linear(32*7*7, 10) # 全连接层 7*7*32, num_classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # 卷一次\n",
    "        x = self.conv2(x) # 卷两次\n",
    "        x = x.view(x.size(0), -1) #flat (batch_size, 32*7*7) \n",
    "        # 将前面多维度的tensor展平成一维 x.size(0)指batchsize的值\n",
    "        # view()函数的功能根reshape类似，用来转换size大小\n",
    "        output = self.out(x) # fc out全连接层 分类器\n",
    "        return output\n",
    "# 定义网络结构\n",
    " \n",
    " \n",
    "# 查看网络结构\n",
    "cnn = CNN()\n",
    "print(cnn) # 使用print(cnn)可以看到网络的结构详细信息，可以看到ReLU()也是一层layer\n",
    "# 查看网络结构\n",
    " \n",
    " \n",
    "# 训练 需要特别指出的是记得每次反向传播前都要清空上一次的梯度，optimizer.zero_grad()\n",
    "# optimizer 可以指定程序优化特定的选项，例如学习速率，权重衰减等\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) # torch.optim是一个实现了多种优化算法的包\n",
    " \n",
    "# loss_fun CrossEntropyLoss 交叉熵损失\n",
    "# 信息量：它是用来衡量一个事件的不确定性的；一个事件发生的概率越大，不确定性越小，则它所携带的信息量就越小。\n",
    "# 熵：它是用来衡量一个系统的混乱程度的，代表一个系统中信息量的总和；信息量总和越大，表明这个系统不确定性就越大。\n",
    "# 交叉熵：它主要刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近\n",
    "loss_func = nn.CrossEntropyLoss() # 该损失函数结合了nn.LogSoftmax()和nn.NLLLoss()两个函数 适用于分类\n",
    " \n",
    "# training loop\n",
    "for epoch in range(EPOCH):\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        batch_x = Variable(x)\n",
    "        batch_y = Variable(y)\n",
    "        output = cnn(batch_x) # 输入训练数据\n",
    "        loss = loss_func(output, batch_y) # 计算误差 #　实际输出，　期望输出\n",
    "        optimizer.zero_grad() # 清空上一次梯度\n",
    "        loss.backward() # 误差反向传递 只需要调用.backward()即可\n",
    "        optimizer.step() # cnn的优化器参数更新\n",
    "# 训练\n",
    " \n",
    " \n",
    "# 预测结果\n",
    "# cnn.eval()\n",
    "test_output = cnn(test_x[:10])\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "# torch.max(input, dim)函数  \n",
    "# torch.max(test_output, 1)[1]  取出来indices 每行最大值的索引\n",
    "# 输入 input是softmax函数输出的一个tensor  \n",
    "# 输入 dim是max函数索引的维度0/1，0是每列的最大值，1是每行的最大值\n",
    "# 输出 函数会返回两个tensor，第一个tensor是每行的最大值；第二个tensor是每行最大值的索引。\n",
    "# squeeze()函数的功能是：从矩阵shape中，去掉维度为1的。例如一个矩阵是的shape是（5， 1），使用过这个函数后，结果为（5，）。\n",
    "print(pred_y, 'prediction number')\n",
    "print(test_y[:10], 'real number')\n",
    "# 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xugx\\AppData\\Local\\Temp\\ipykernel_11160\\714208242.py:33: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000]/255.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MNIST' object has no attribute 'test_lables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\My_work_file\\Program_file\\VScode_Pro\\python_pro\\Graduation_Design\\DL\\demo\\manual_seed.ipynb 单元格 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m test_x \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39munsqueeze(test_data\u001b[39m.\u001b[39mtest_data, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), volatile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mFloatTensor)[:\u001b[39m2000\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m255.\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# torch.unsqueeze 返回一个新的张量，对输入的既定位置插入维度 1\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m test_y \u001b[39m=\u001b[39m test_data\u001b[39m.\u001b[39;49mtest_lables[:\u001b[39m2000\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W2sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# 数据预处理\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W2sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m  \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W2sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m  \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W2sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# 4）需要·实现·forward()方法，用于网络的前向传播，而反向传播只需要·调用·Variable.backward()即可。\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W2sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# 输入的四维张量[N, C, H, W]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/My_work_file/Program_file/VScode_Pro/python_pro/Graduation_Design/DL/demo/manual_seed.ipynb#W2sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCNN\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MNIST' object has no attribute 'test_lables'"
     ]
    }
   ],
   "source": [
    "import torch # 需要的各种包\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision # 数据库模块\n",
    " \n",
    " \n",
    "# 数据预处理\n",
    "# 将training data转化成torch能够使用的DataLoader，这样可以方便使用batch进行训练\n",
    "torch.manual_seed(1) # reproducible 将随机数生成器的种子设置为固定值，这样，当调用时torch.rand(x)，结果将可重现\n",
    " \n",
    "# Hyper Parameters\n",
    "EPOCH = 1 # 训练迭代次数\n",
    "BATCH_SIZE = 50 # 分块送入训练器\n",
    "LR = 0.001 # 学习率 learning rate\n",
    " \n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist/', # 保存位置 若没有就新建\n",
    "    train=True, # training set\n",
    "    transform=torchvision.transforms.ToTensor(), # \n",
    "    # converts a PIL.Image or numpy.ndarray to torch.FloatTensor(C*H*W) in range(0.0,1.0)\n",
    "    download=True\n",
    ")\n",
    " \n",
    "test_data = torchvision.datasets.MNIST(root='./MNIST/')\n",
    " \n",
    "# 如果是普通的Tensor数据，想使用 torch_dataset = data.TensorDataset(data_tensor=x, target_tensor=y)\n",
    "# 将Tensor转换成torch能识别的dataset\n",
    "# 批训练， 50 samples, 1 channel, 28*28, (50, 1, 28 ,28)\n",
    "train_loader = data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    " \n",
    "test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000]/255.\n",
    "# torch.unsqueeze 返回一个新的张量，对输入的既定位置插入维度 1\n",
    " \n",
    "test_y = test_data.test_lables[:2000]\n",
    "# 数据预处理\n",
    " \n",
    " \n",
    "# 定义网络结构\n",
    "# 1）class CNN需要·继承·Module \n",
    "# 2）需要·调用·父类的构造方法：super(CNN, self).__init__()\n",
    "# 3）在Pytorch中激活函数Relu也算是一层layer\n",
    "# 4）需要·实现·forward()方法，用于网络的前向传播，而反向传播只需要·调用·Variable.backward()即可。\n",
    "# 输入的四维张量[N, C, H, W]\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # nn.Sequential一个有序的容器，神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行，\n",
    "        # 同时以神经网络模块为元素的有序字典也可以作为传入参数\n",
    "        # nn.Conv2d 二维卷积 先实例化再使用 在Pytorch的nn模块中，它是不需要你手动定义网络层的权重和偏置的\n",
    "        self.conv1 = nn.Sequential( #input shape (1,28,28)\n",
    "            nn.Conv2d(in_channels=1, #input height 必须手动提供 输入张量的channels数\n",
    "                      out_channels=16, #n_filter 必须手动提供 输出张量的channels数\n",
    "                      kernel_size=5, #filter size 必须手动提供 卷积核的大小 \n",
    "                      # 如果左右两个数不同，比如3x5的卷积核，那么写作kernel_size = (3, 5)，注意需要写一个tuple，而不能写一个列表（list）\n",
    "                      stride=1, #filter step 卷积核在图像窗口上每次平移的间隔，即所谓的步长\n",
    "                      padding=2 #con2d出来的图片大小不变 Pytorch与Tensorflow在卷积层实现上最大的差别就在于padding上\n",
    "            ), # output shape (16,28,28) 输出图像尺寸计算公式是唯一的 # O = （I - K + 2P）/ S +1\n",
    "            nn.ReLU(), # 分段线性函数，把所有的负值都变为0，而正值不变，即单侧抑制\n",
    "            nn.MaxPool2d(kernel_size=2) #2x2采样，28/2=14，output shape (16,14,14) maxpooling有局部不变性而且可以提取显著特征的同时降低模型的参数，从而降低模型的过拟合\n",
    "        ) \n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, 5, 1, 2), #output shape (32,7,7)\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(2))\n",
    "        # 因上述几层网络处理后的output为[32,7,7]的tensor，展开即为7*7*32的一维向量，接上一层全连接层，最终output_size应为10，即识别出来的数字总类别数\n",
    "        # 在二维图像处理的任务中，全连接层的输入与输出一般都设置为二维张量，形状通常为[batch_size, size]\n",
    "        self.out = nn.Linear(32*7*7, 10) # 全连接层 7*7*32, num_classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # 卷一次\n",
    "        x = self.conv2(x) # 卷两次\n",
    "        x = x.view(x.size(0), -1) #flat (batch_size, 32*7*7) \n",
    "        # 将前面多维度的tensor展平成一维 x.size(0)指batchsize的值\n",
    "        # view()函数的功能根reshape类似，用来转换size大小\n",
    "        output = self.out(x) # fc out全连接层 分类器\n",
    "        return output\n",
    "# 定义网络结构\n",
    " \n",
    " \n",
    "# 查看网络结构\n",
    "cnn = CNN()\n",
    "print(cnn) # 使用print(cnn)可以看到网络的结构详细信息，可以看到ReLU()也是一层layer\n",
    "# 查看网络结构\n",
    " \n",
    " \n",
    "# 训练 需要特别指出的是记得每次反向传播前都要清空上一次的梯度，optimizer.zero_grad()\n",
    "# optimizer 可以指定程序优化特定的选项，例如学习速率，权重衰减等\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) # torch.optim是一个实现了多种优化算法的包\n",
    " \n",
    "# loss_fun CrossEntropyLoss 交叉熵损失\n",
    "# 信息量：它是用来衡量一个事件的不确定性的；一个事件发生的概率越大，不确定性越小，则它所携带的信息量就越小。\n",
    "# 熵：它是用来衡量一个系统的混乱程度的，代表一个系统中信息量的总和；信息量总和越大，表明这个系统不确定性就越大。\n",
    "# 交叉熵：它主要刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近\n",
    "loss_func = nn.CrossEntropyLoss() # 该损失函数结合了nn.LogSoftmax()和nn.NLLLoss()两个函数 适用于分类\n",
    " \n",
    "# training loop\n",
    "for epoch in range(EPOCH):\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        batch_x = Variable(x)\n",
    "        batch_y = Variable(y)\n",
    "        output = cnn(batch_x) # 输入训练数据\n",
    "        loss = loss_func(output, batch_y) # 计算误差 #　实际输出，　期望输出\n",
    "        optimizer.zero_grad() # 清空上一次梯度\n",
    "        loss.backward() # 误差反向传递 只需要调用.backward()即可\n",
    "        optimizer.step() # cnn的优化器参数更新\n",
    "# 训练\n",
    " \n",
    " \n",
    "# 预测结果\n",
    "# cnn.eval()\n",
    "test_output = cnn(test_x[:10])\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "# torch.max(input, dim)函数  \n",
    "# torch.max(test_output, 1)[1]  取出来indices 每行最大值的索引\n",
    "# 输入 input是softmax函数输出的一个tensor  \n",
    "# 输入 dim是max函数索引的维度0/1，0是每列的最大值，1是每行的最大值\n",
    "# 输出 函数会返回两个tensor，第一个tensor是每行的最大值；第二个tensor是每行最大值的索引。\n",
    "# squeeze()函数的功能是：从矩阵shape中，去掉维度为1的。例如一个矩阵是的shape是（5， 1），使用过这个函数后，结果为（5，）。\n",
    "print(pred_y, 'prediction number')\n",
    "print(test_y[:10], 'real number')\n",
    "# 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建神经网络\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()   # 继承__init__功能\n",
    "        # 第一层卷积\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # 输入[3,5,5]\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,    # 输入图片的高度 3通道\n",
    "                out_channels=32,   # 输出图片的高度 32个核\n",
    "                kernel_size=3,    # 3x3的卷积核，相当于过滤器\n",
    "                stride=1,         # 卷积核在图上滑动，每隔一个扫一次\n",
    "                padding=1,        # 给图外边补上1圈0\n",
    "            ),\n",
    "            # 经过卷积层 输出[32,5,5] 传入池化层\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2)   # 经过池化 输出[15,2,2] 传入下一个卷积\n",
    "        )\n",
    "        # 第2层卷积\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # 输入[,5,5]\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,    # 输入图片的高度 3通道\n",
    "                out_channels=32,   # 输出图片的高度 32个核\n",
    "                kernel_size=3,    # 3x3的卷积核，相当于过滤器\n",
    "                stride=1,         # 卷积核在图上滑动，每隔一个扫一次\n",
    "                padding=0,        # 给图外边补上0圈0\n",
    "            ),\n",
    "            # 经过卷积层 输出[15,4,4] 传入池化层\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)   # 经过池化 输出[15,2,2] 传入下一个卷积\n",
    "        )\n",
    "        ## 输出层\n",
    "        self.output = nn.Linear(in_features=15*2*2, out_features=6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)           # [batch, 32,7,7]\n",
    "        x = x.view(x.size(0), -1)   # 保留batch, 将后面的乘到一起 [batch, 32*7*7]\n",
    "        output = self.output(x)     # 输出[50,10]\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
